Summary:Propose a parameter sever framework for distributed machine learning problem.
It is a framework manages asynchronous data communication between nodes.
First, need to demonstrate the scalability of the the framework.
To illustrate "learning tasks are often performed in a unreliable cloud environment".(three months experiment from a company here)
------
Flexible consistency models:Our parameter server provides five key features:1.Efficient communication 2.Flexible consistency models
3.Elastic Scalability.4.Fault Tolerance and Durability 5.Ease of Use
 Engineering Challenges1.Communication2.Fault tolerance
 Related Work-YahooLDA-Distbelief-GraphLab/. In a sense, a core goal of theparameter server framework is to capture the benefits ofGraphLab’s asynchrony without its structural limitations.
feature-extraction, the objective function, and learning.
To motivate the design decisions in our system, next we briefly outline the two widely used machine learning
technologies that we will use to demonstrate the efficacy
of our parameter server.
Risk Minimization
The most expensive step in Algorithm 1 is computing
the subgradient to update w.
Architecture: 3.1(Key,Value) Vectors 3.2 Range Push and Pull 3.3 User-Defined Functions on the Server 3.4  Asynchronous Tasks and Dependency
3.5 Flexible Consistency. 3.6 User-defined Filters
 Implementation
4.1 Vector Clock 4.2 Messages 4.3 Consistent Hashing 4.4 Replication and Consistency 4.5 Server Management 4.6 Worker Management
5 Evaluation We evaluate our parameter server based on the use cases of Section 2 — Sparse Logistic Regression and Latent
Dirichlet Allocation. 

Summary: 
We described a parameter server framework to solve distributed machine learning problems. This framework is
easy to use: Globally shared parameters can be used as
local sparse vectors or matrices to perform linear algebra
operations with local training data. It is efficient: All communication is asynchronous. Flexible consistentcy models are supported to balance the trade-off between system
efficiency and fast algorithm convergence rate. Furthermore, it provides elastic scalability and fault tolerance,
aiming for stable long term deployment. Finally, we show
experiments for several challenging tasks on real datasets
with billions of variables to demonstrate its efficiency. We
believe that this third generation parameter server is an
important building block for scalable machine learning.


April 5th : Summary: TensorFlow: A System for Large-Scale Machine Learning 
In this paper, we describe the TensorFlow dataflow model
and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.
In this paper, we focus on
neural network training as a challenging systems problem,
and select two representative applications from this space:
image classification and language modeling.

